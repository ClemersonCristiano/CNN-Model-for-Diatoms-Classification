{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmqXBlVSwZbK",
        "outputId": "10341bfd-1916-437c-d0f4-725ef9450010"
      },
      "outputs": [],
      "source": [
        "# Monta o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define o caminho para o arquivo zip no seu Drive\n",
        "# Substitua 'caminho/para/seu/arquivo.zip' pelo caminho real do seu arquivo\n",
        "zip_file_path = '/content/drive/MyDrive/dataset/dataset.zip'\n",
        "\n",
        "# Define o diretório de destino para extrair os arquivos\n",
        "extract_path = '/content/'\n",
        "\n",
        "# Cria o diretório de destino se ele não existir\n",
        "import os\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Descompacta o arquivo zip\n",
        "# Usa um comando shell para descompactar. O '!' permite executar comandos shell no Colab.\n",
        "print(f\"Descompactando {zip_file_path} para {extract_path}...\")\n",
        "!unzip -q \"{zip_file_path}\" -d \"{extract_path}\"\n",
        "print(\"Descompactação concluída.\")\n",
        "\n",
        "# Agora você pode acessar os arquivos descompactados no diretório especificado (por exemplo, '/content/dataset')\n",
        "# Exemplo: listar o conteúdo do diretório extraído\n",
        "print(f\"\\nConteúdo do diretório descompactado ({extract_path}):\")\n",
        "!ls \"{extract_path}\"\n",
        "\n",
        "# Você pode agora usar a variável 'extract_path' nas células subsequentes\n",
        "# para referenciar a localização dos seus dados. Por exemplo, para definir DATASET_DIR:\n",
        "# DATASET_DIR = extract_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvrcxwJFGQvE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a3bc20e",
        "outputId": "5865b46c-9663-4172-9410-34e0fdb744e5"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL44z3OXHAkC"
      },
      "outputs": [],
      "source": [
        "DATASET_DIR = \"/content/dataset\"\n",
        "\n",
        "CLASSES = ['Encyonema', 'Eunotia', 'Gomphonema', 'Navicula', 'Pinnularia']\n",
        "\n",
        "# Mapeamento de nome de classe para um número (label)\n",
        "class_to_int = {class_name: i for i, class_name in enumerate(CLASSES)}\n",
        "int_to_class = {i: class_name for i, class_name in enumerate(CLASSES)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9gJUD1THDjd"
      },
      "outputs": [],
      "source": [
        "def get_file_lists_and_groups(dataset_dir, classes):\n",
        "    \"\"\"\n",
        "    Mapeia todos os arquivos, extrai seus labels e seus \"grupos de família\".\n",
        "    \"\"\"\n",
        "    filepaths = []  # Lista de todos os caminhos de arquivo\n",
        "    labels = []     # Lista de labels (0, 1, 2, 3, 4)\n",
        "    groups = []     # Lista de \"IDs de família\" (o base_filename)\n",
        "\n",
        "    # Regex para extrair o \"base_filename\"\n",
        "    # Ele captura (grupo 1) o nome base e (grupo 2) o sufixo de augmentação\n",
        "    # Ex: \"Gomphonema_..._timestamp_horizontal.png\"\n",
        "    # Grupo 1: \"Gomphonema_..._timestamp\"\n",
        "    # Grupo 2: \"_horizontal\"\n",
        "    # Ex: \"Gomphonema_..._timestamp.png\"\n",
        "    # Grupo 1: \"Gomphonema_..._timestamp\"\n",
        "    # Grupo 2: None (ou string vazia)\n",
        "    pattern = re.compile(\n",
        "        r'(.+?)(_horizontal|_vertical|_90_graus|_270_graus)?\\.(png|jpg|jpeg|bmp|tiff)$',\n",
        "        re.IGNORECASE # Ignora maiúsculas/minúsculas\n",
        "    )\n",
        "\n",
        "    print(f\"Mapeando arquivos em '{dataset_dir}'...\")\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(dataset_dir, class_name)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            print(f\"[AVISO] Pasta não encontrada: {class_dir}\")\n",
        "            continue\n",
        "\n",
        "        label = class_to_int[class_name]\n",
        "\n",
        "        for filename in os.listdir(class_dir):\n",
        "            match = pattern.match(filename)\n",
        "\n",
        "            if match:\n",
        "                # O \"base_filename\" é nosso ID de grupo (família)\n",
        "                base_filename = match.group(1)\n",
        "                filepath = os.path.join(class_dir, filename)\n",
        "\n",
        "                filepaths.append(filepath)\n",
        "                labels.append(label)\n",
        "                groups.append(base_filename)\n",
        "\n",
        "    print(f\"Mapeamento concluído. {len(filepaths)} arquivos encontrados.\")\n",
        "    return np.array(filepaths), np.array(labels), np.array(groups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9htQeYqHKuL",
        "outputId": "91618ccc-edbd-460e-9ef3-4e279a110627"
      },
      "outputs": [],
      "source": [
        "# --- 2. Execução do Mapeamento ---\n",
        "\n",
        "all_filepaths, all_labels, all_groups = get_file_lists_and_groups(DATASET_DIR, CLASSES)\n",
        "\n",
        "# --- 3. Divisão por Grupo (Família) ---\n",
        "\n",
        "print(\"\\nIniciando divisão Treino/Validação por 'Família'...\")\n",
        "\n",
        "# Queremos 1 divisão (n_splits=1) com 20% dos *grupos* para teste/validação\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "# gss.split() nos dá os *índices* dos arrays para treino e validação\n",
        "# Usamos 'all_filepaths' como X (só para ter o tamanho), e 'all_groups' como 'groups'\n",
        "train_idx, val_idx = next(gss.split(all_filepaths, groups=all_groups))\n",
        "\n",
        "# --- 4. Criação das Listas Finais ---\n",
        "\n",
        "# Selecionamos os arquivos e labels com base nos índices\n",
        "train_files = all_filepaths[train_idx]\n",
        "train_labels = all_labels[train_idx]\n",
        "\n",
        "val_files = all_filepaths[val_idx]\n",
        "val_labels = all_labels[val_idx]\n",
        "\n",
        "print(\"Divisão concluída.\")\n",
        "print(f\"  Imagens de Treino:    {len(train_files)}\")\n",
        "print(f\"  Imagens de Validação: {len(val_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41-9gXGsHPmz",
        "outputId": "cd33ab57-f063-411d-ca41-82e1bfd16ae2"
      },
      "outputs": [],
      "source": [
        "# --- 5. Verificação de Vazamento (Data Leakage) ---\n",
        "\n",
        "# Para confirmar, vamos verificar se algum 'base_filename'\n",
        "# existe em *ambos* os conjuntos. A interseção deve ser 0.\n",
        "\n",
        "train_groups_set = set(all_groups[train_idx])\n",
        "val_groups_set = set(all_groups[val_idx])\n",
        "\n",
        "leakage = train_groups_set.intersection(val_groups_set)\n",
        "\n",
        "if not leakage:\n",
        "    print(\"\\n[SUCESSO] Verificação de vazamento concluída. Nenhuma família de imagens vazou entre os conjuntos de Treino e Validação.\")\n",
        "else:\n",
        "    print(f\"\\n[ERRO] Verificação de vazamento falhou! {len(leakage)} famílias estão em ambos os conjuntos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BVO-iSyHP5N",
        "outputId": "1f7416c3-99e8-46eb-9f08-517bd4a00374"
      },
      "outputs": [],
      "source": [
        "# --- 6. Cálculo dos Pesos de Classe (Class Weights) ---\n",
        "\n",
        "print(\"\\nCalculando pesos de classe para lidar com desbalanceamento...\")\n",
        "\n",
        "# É importante calcular os pesos com base APENAS nos dados de TREINO,\n",
        "# pois é neles que o modelo será treinado.\n",
        "y_integers = train_labels\n",
        "\n",
        "# Obter as classes únicas presentes nos dados de treino\n",
        "classes_unicas = np.unique(y_integers)\n",
        "print(f\"Classes únicas encontradas nos dados de treino: {classes_unicas}\")\n",
        "\n",
        "# Calcular os pesos\n",
        "# O modo 'balanced' faz o cálculo automaticamente: N_amostras / (N_classes * N_amostras_por_classe)\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes_unicas,\n",
        "    y=y_integers\n",
        ")\n",
        "\n",
        "# Criar o dicionário de pesos que o Keras espera (ex: {0: 1.5, 1: 0.8, ...})\n",
        "class_weights = dict(zip(classes_unicas, class_weights_array))\n",
        "\n",
        "print(\"Cálculo de pesos concluído.\")\n",
        "print(\"Estes pesos serão usados para penalizar erros em classes minoritárias:\")\n",
        "\n",
        "# Imprimir os pesos de forma legível\n",
        "# Lembre-se: int_to_class = {0: 'Encyonema', 1: 'Eunotia', ...}\n",
        "# (Se você não tiver o 'int_to_class', pode imprimir o dicionário 'class_weights' diretamente)\n",
        "for class_int, weight in class_weights.items():\n",
        "    class_name = int_to_class.get(class_int, \"Classe Desconhecida\")\n",
        "    print(f\"  -> Classe {class_int} ({class_name}): Peso = {weight:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF_0olwwHQIe",
        "outputId": "9b818317-268f-48ed-b417-f2e925559f57"
      },
      "outputs": [],
      "source": [
        "# --- 7. Definição do Pipeline de Dados (com Augmentação e Rotação) ---\n",
        "\n",
        "# Parâmetros\n",
        "IMAGE_SIZE = 400\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE # Otimização do TensorFlow\n",
        "\n",
        "def load_image(filepath, label):\n",
        "    \"\"\"\n",
        "    Carrega, decodifica, converte (Grayscale->RGB) e redimensiona.\n",
        "    Saída: pixels no intervalo [0, 255]\n",
        "    \"\"\"\n",
        "    image = tf.io.read_file(filepath)\n",
        "    image = tf.io.decode_png(image, channels=1)\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "    return image, label\n",
        "\n",
        "def augment_image(image, label):\n",
        "    \"\"\"\n",
        "    Aplica augmentações aleatórias em tempo real.\n",
        "    Entrada/Saída: pixels no intervalo [0, 255]\n",
        "    \"\"\"\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    k_rot = tf.random.uniform(shape=(), minval=0, maxval=4, dtype=tf.int32)\n",
        "    image = tf.image.rot90(image, k=k_rot)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1 * 255.0)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
        "    return image, label\n",
        "\n",
        "def normalize_image(image, label):\n",
        "    \"\"\"\n",
        "    Normaliza a imagem para o formato que a ResNetV2 espera [-1, 1].\n",
        "    \"\"\"\n",
        "    image = tf.keras.applications.resnet_v2.preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "def create_dataset(filepaths, labels, is_training=True):\n",
        "    \"\"\"\n",
        "    Cria um objeto tf.data.Dataset completo.\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "\n",
        "    if is_training:\n",
        "        # 1. Embaralhar os FILEPATHS (strings), o que é leve para a RAM.\n",
        "        dataset = dataset.shuffle(buffer_size=len(filepaths), reshuffle_each_iteration=True)\n",
        "\n",
        "    # 2. Carregar as imagens (agora em ordem aleatória)\n",
        "    dataset = dataset.map(load_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if is_training:\n",
        "        # 3. Aplicar a augmentação\n",
        "        dataset = dataset.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # 4. Agrupar em lotes (Batch)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    # 5. Normalizar (depois do batch, é mais rápido na GPU)\n",
        "    dataset = dataset.map(normalize_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # 6. Otimização: Pré-carregar o próximo lote\n",
        "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "print(\"\\nCriando pipelines de dados com AUGMENTAÇÃO ONLINE...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRW_H_JaHQTt",
        "outputId": "2cbabc18-deeb-4edb-c815-81754f38120b"
      },
      "outputs": [],
      "source": [
        "# --- 8. Criar os Datasets Finais ---\n",
        "\n",
        "# Converter labels para o formato correto (one-hot encoding)\n",
        "train_labels_one_hot = tf.keras.utils.to_categorical(train_labels, num_classes=len(CLASSES))\n",
        "val_labels_one_hot = tf.keras.utils.to_categorical(val_labels, num_classes=len(CLASSES))\n",
        "\n",
        "# Criar os pipelines\n",
        "train_dataset = create_dataset(train_files, train_labels_one_hot, is_training=True)\n",
        "val_dataset = create_dataset(val_files, val_labels_one_hot, is_training=False) # Validação NUNCA é aumentada\n",
        "\n",
        "print(\"Pipelines criados com sucesso.\")\n",
        "print(f\"  -> train_dataset (com augmentação): {train_dataset}\")\n",
        "print(f\"  -> val_dataset (sem augmentação):   {val_dataset}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDVMDRk4HiIq"
      },
      "outputs": [],
      "source": [
        "# --- 9. Definição da Arquitetura do Modelo ---\n",
        "\n",
        "print(\"\\nConstruindo o modelo de Transfer Learning (ResNet50V2)...\")\n",
        "\n",
        "INPUT_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS) # (400, 400, 3)\n",
        "NUM_CLASSES = len(CLASSES) # 5\n",
        "\n",
        "# --- Parte 1: Carregar a Base Pré-Treinada ---\n",
        "\n",
        "# Carregamos a ResNet50V2, treinada no ImageNet\n",
        "# include_top=False: remove a camada final original (que classificava 1000 classes)\n",
        "base_model = ResNet50V2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=INPUT_SHAPE\n",
        ")\n",
        "\n",
        "# --- Parte 2: Congelar a Base ---\n",
        "\n",
        "# \"Congelar\" os pesos da base.\n",
        "# Não queremos re-treiná-los na primeira fase (Feature Extraction).\n",
        "base_model.trainable = False\n",
        "\n",
        "# --- Parte 3: Construir o \"Head\" (Nosso Classificador) ---\n",
        "\n",
        "# 1. Definir a entrada do nosso modelo\n",
        "inputs = Input(shape=INPUT_SHAPE)\n",
        "\n",
        "# 2. Passar a entrada pela base (em modo \"inferência\")\n",
        "# training=False garante que as camadas de BatchNormalization da base\n",
        "# usem suas estatísticas salvas e não tentem se atualizar.\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# 3. Adicionar nossas camadas no topo\n",
        "# GlobalAveragePooling2D achata a saída 4D da ResNet em um vetor 1D\n",
        "x = GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n",
        "# Adicionamos uma camada densa para aprender os padrões específicos das diatomáceas\n",
        "x = Dense(256, activation='relu', name='dense_head')(x)\n",
        "# Dropout é uma técnica de regularização crucial para evitar overfitting\n",
        "x = Dropout(0.5)(x)\n",
        "# Camada de saída final. 'softmax' para classificação multiclasse\n",
        "outputs = Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n",
        "\n",
        "# --- Parte 4: Criar o Modelo Final ---\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# --- Parte 5: Compilar o Modelo ---\n",
        "model.compile(\n",
        "    # Adam é um otimizador robusto. 0.001 é um bom learning rate inicial\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    # 'categorical_crossentropy' é a loss function correta\n",
        "    # porque usamos to_categorical (one-hot) em nossos labels.\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'] # Vamos monitorar a acurácia\n",
        ")\n",
        "\n",
        "print(\"Modelo construído e compilado com sucesso.\")\n",
        "\n",
        "# --- 10. Exibir Resumo do Modelo ---\n",
        "print(\"\\nResumo do modelo:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-As6OflDHo6s"
      },
      "outputs": [],
      "source": [
        "# --- 11. Configuração dos Callbacks ---\n",
        "\n",
        "print(\"\\nConfigurando callbacks (ModelCheckpoint e EarlyStopping)...\")\n",
        "\n",
        "# Define o caminho para salvar o melhor modelo\n",
        "best_model_path = \"/content/drive/MyDrive/dataset/diatom_classifier_best_model_puroTratado.keras\"\n",
        "\n",
        "# 1. ModelCheckpoint: Salva o modelo com a melhor acurácia de validação\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=best_model_path,\n",
        "    save_best_only=True,       # Salva apenas se for melhor que o anterior\n",
        "    monitor='val_accuracy',    # Monitora a acurácia da validação\n",
        "    mode='max',                # Queremos maximizar a acurácia\n",
        "    verbose=1                  # Imprime uma mensagem quando salva\n",
        ")\n",
        "\n",
        "# 2. EarlyStopping: Para o treinamento se não houver melhoria\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',        # Monitora a perda da validação\n",
        "    patience=3,                # Número de épocas sem melhoria antes de parar (3 é um bom começo)\n",
        "    mode='min',                # Queremos minimizar a perda\n",
        "    verbose=1,\n",
        "    restore_best_weights=True  # Restaura os pesos da melhor época ao final\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFtMZF5gt2Sn"
      },
      "outputs": [],
      "source": [
        "# --- Função de Plotagem das Curvas de Treinamento ---\n",
        "\n",
        "def plot_training_curves(history, title_suffix):\n",
        "    \"\"\"\n",
        "    Plota as curvas de Acurácia e Perda do treinamento.\n",
        "\n",
        "    Argumentos:\n",
        "        history (tf.keras.callbacks.History): O objeto retornado por model.fit().\n",
        "        title_suffix (str): O sufixo para o título (ex: \"Fase 1\" ou \"Fase 2\").\n",
        "    \"\"\"\n",
        "    print(f\"\\nGerando curvas de treinamento para: {title_suffix}...\")\n",
        "\n",
        "    # Pega as métricas do histórico\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    # Gráfico de Acurácia\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'bo-', label='Acurácia (Treino)')\n",
        "    plt.plot(epochs, val_acc, 'ro-', label='Acurácia (Validação)')\n",
        "    plt.title(f'Curva de Acurácia - {title_suffix}')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Gráfico de Perda (Loss)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'bo-', label='Perda (Treino)')\n",
        "    plt.plot(epochs, val_loss, 'ro-', label='Perda (Validação)')\n",
        "    plt.title(f'Curva de Perda - {title_suffix}')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.suptitle(f'Curvas de Treinamento - {MODEL_NAME}', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Oc5wY0PYK8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 12. Início do Treinamento (Fase 1) ---\n",
        "\n",
        "print(\"\\n--- INICIANDO TREINAMENTO (FASE 1: FEATURE EXTRACTION) ---\")\n",
        "\n",
        "NUM_EPOCHS = 20  # Vamos começar com 20 épocas para esta fase\n",
        "BATCH_SIZE = 32  # O BATCH_SIZE que definimos no pipeline\n",
        "\n",
        "# Calcular os \"steps\" (passos) por época.\n",
        "# Isso é necessário ao usar tf.data\n",
        "steps_per_epoch = math.ceil(len(train_files) / BATCH_SIZE)\n",
        "validation_steps = math.ceil(len(val_files) / BATCH_SIZE)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    class_weight=class_weights,  # <-- Aplicando os pesos contra o desbalanceamento!\n",
        "    callbacks=[model_checkpoint, early_stopping],\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n",
        "print(\"\\n--- TREINAMENTO (FASE 1) CONCLUÍDO ---\")\n",
        "print(f\"O melhor modelo foi salvo em: {best_model_path}\")\n",
        "\n",
        "try:\n",
        "    plot_training_curves(history, f\"Fase 1 - {MODEL_NAME}\")\n",
        "except NameError:\n",
        "    plot_training_curves(history, \"Fase 1: Extração de Features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2_9sUyX3vTP"
      },
      "outputs": [],
      "source": [
        "# --- 13. Carregar o Modelo da Fase 1 ---\n",
        "\n",
        "print(\"\\n--- INICIANDO FASE 2: AJUSTE FINO (FINE-TUNING) ---\")\n",
        "print(\"Carregando o melhor modelo da Fase 1...\")\n",
        "\n",
        "# Carrega o modelo que atingiu melhor acurácia\n",
        "# Keras também carrega o estado do otimizador, mas vamos recompilar.\n",
        "model = load_model('/content/drive/MyDrive/dataset/diatom_classifier_best_model_puroTratado.keras')\n",
        "\n",
        "# --- 14. \"Descongelar\" a Base ---\n",
        "\n",
        "# Precisamos acessar a \"base_model\" (a ResNet) dentro do nosso modelo salvo\n",
        "# O nome 'resnet50v2' é o nome padrão que Keras deu a ela (vimos no model.summary())\n",
        "try:\n",
        "    base_model = model.get_layer('resnet50v2')\n",
        "    base_model.trainable = True # <-- A MÁGICA ACONTECE AQUI\n",
        "    print(f\"Camada '{base_model.name}' foi descongelada e está pronta para o ajuste fino.\")\n",
        "except ValueError:\n",
        "    print(\"ERRO: Não foi possível encontrar a camada 'resnet50v2'. Verifique o model.summary() da Fase 1.\")\n",
        "    # Se o nome for diferente, ajuste-o.\n",
        "\n",
        "# --- 15. Recompilar com Taxa de Aprendizado Baixíssima ---\n",
        "\n",
        "# Este é o passo MAIS CRÍTICO do ajuste fino.\n",
        "# Usamos uma taxa de aprendizado 100x a 1000x menor que antes.\n",
        "# Queremos \"ajustar\" os pesos, não \"destruí-los\".\n",
        "LEARNING_RATE_PHASE_2 = 0.00001 # (1e-5)\n",
        "\n",
        "model.compile(\n",
        "    # Adam com um learning rate muito baixo\n",
        "    optimizer=Adam(learning_rate=LEARNING_RATE_PHASE_2),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f\"Modelo recompilado para ajuste fino com learning rate de {LEARNING_RATE_PHASE_2}.\")\n",
        "\n",
        "# Vamos verificar o resumo. Agora, TODOS os parâmetros devem ser \"Trainable\".\n",
        "print(\"\\nResumo do modelo para Fase 2:\")\n",
        "model.summary()\n",
        "\n",
        "# --- 16. Configurar Callbacks para a Fase 2 ---\n",
        "\n",
        "# Define o caminho para salvar o melhor modelo no seu Drive\n",
        "best_model_path_phase2 = \"/content/drive/MyDrive/dataset/diatom_classifier_best_model_puroTratado_finetuned.keras\"\n",
        "\n",
        "# 1. ModelCheckpoint para a Fase 2\n",
        "model_checkpoint_phase2 = ModelCheckpoint(\n",
        "    filepath=best_model_path_phase2,\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 2. EarlyStopping para a Fase 2 (com paciência, pois as melhorias são lentas)\n",
        "early_stopping_phase2 = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,  # 3 épocas sem melhoria na perda de validação\n",
        "    mode='min',\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaTniCgsTe-P"
      },
      "outputs": [],
      "source": [
        "# --- 17. Início do Treinamento (Fase 2) Fine-Tuning ---\n",
        "\n",
        "print(\"\\n--- CONTINUANDO TREINAMENTO (FASE 2: AJUSTE FINO) ---\")\n",
        "\n",
        "# Vamos treinar por mais épocas. O modelo continuará de onde parou.\n",
        "NUM_EPOCHS_PHASE_2 = 20 # Vamos dar mais 10 épocas de chance\n",
        "BATCH_SIZE = 32 # O mesmo BATCH_SIZE\n",
        "\n",
        "# Reutilizar os steps calculados na Fase 1\n",
        "# (Assumindo que 'train_files' e 'val_files' ainda estão na memória)\n",
        "try:\n",
        "    steps_per_epoch = math.ceil(len(train_files) / BATCH_SIZE)\n",
        "    validation_steps = math.ceil(len(val_files) / BATCH_SIZE)\n",
        "except NameError:\n",
        "    print(\"Recalculando steps (variáveis não encontradas)...\")\n",
        "    # Se você estiver em um novo script, precisará recriar as listas\n",
        "    # de arquivos para obter o len() delas.\n",
        "    # Por segurança (assumindo que você está no mesmo notebook):\n",
        "    steps_per_epoch = 551\n",
        "    validation_steps = 138\n",
        "\n",
        "history_phase2 = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=NUM_EPOCHS_PHASE_2,\n",
        "    validation_data=val_dataset,\n",
        "    class_weight=class_weights,  # <-- Ainda usamos os pesos de classe!\n",
        "    callbacks=[model_checkpoint_phase2, early_stopping_phase2],\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps\n",
        "    # O modelo magicamente \"continua\" porque carregamos seus pesos.\n",
        ")\n",
        "\n",
        "print(\"\\n--- TREINAMENTO (FASE 2) CONCLUÍDO ---\")\n",
        "print(f\"O melhor modelo de ajuste fino foi salvo em: {best_model_path_phase2}\")\n",
        "\n",
        "try:\n",
        "    plot_training_curves(history_phase2, f\"Fase 2 - {MODEL_NAME}\")\n",
        "except NameError:\n",
        "    plot_training_curves(history_phase2, \"Fase 2: Ajuste Fino\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HBdKOe2khKR"
      },
      "outputs": [],
      "source": [
        "def plot_tsne_visualization(model, val_dataset, y_true_labels, classes, model_name):\n",
        "    \"\"\"\n",
        "    Extrai features (embeddings) do modelo, aplica t-SNE e plota a visualização 2D.\n",
        "\n",
        "    Argumentos:\n",
        "        model (tf.keras.Model): O modelo treinado e carregado.\n",
        "        val_dataset (tf.data.Dataset): O pipeline de dados de validação.\n",
        "        y_true_labels (np.array): O array de labels verdadeiros (como índices, ex: [0, 1, 4...]).\n",
        "        classes (list): A lista de nomes das classes (ex: ['Encyonema', ...]).\n",
        "        model_name (str): O nome do modelo para o título do gráfico.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"--- PASSO 10: Gerando Visualização t-SNE ---\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. Criar o \"extrator de features\"\n",
        "    # Usamos o nome 'global_average_pooling2d' que acabamos de adicionar\n",
        "    try:\n",
        "        feature_layer = model.get_layer('global_average_pooling2d')\n",
        "    except ValueError:\n",
        "        print(\"ERRO: Não foi possível encontrar a camada 'global_average_pooling2d'.\")\n",
        "        print(\"Certifique-se de que você nomeou a camada e re-treinou o modelo.\")\n",
        "        return\n",
        "\n",
        "    # Cria um novo modelo que termina na camada de features\n",
        "    feature_extractor = Model(inputs=model.input, outputs=feature_layer.output)\n",
        "\n",
        "    # 2. Extrair as features (embeddings) de todo o dataset de validação\n",
        "    print(\"Extraindo features (embeddings) do dataset de validação...\")\n",
        "    # A função .predict() irá iterar por todo o val_dataset\n",
        "    features = feature_extractor.predict(val_dataset, verbose=1)\n",
        "    # O resultado será (1468, 2048) - (N_imagens_val, N_features)\n",
        "    print(f\"Features extraídas. Shape: {features.shape}\")\n",
        "\n",
        "    # 3. Rodar o t-SNE\n",
        "    # Isso pode levar alguns minutos.\n",
        "    print(\"Calculando t-SNE (isso pode levar alguns minutos)...\")\n",
        "    tsne = TSNE(n_components=2,       # Reduzir para 2 dimensões (x, y)\n",
        "                perplexity=30.0,    # Valor padrão\n",
        "                n_iter=1000,        # Iterações\n",
        "                random_state=42,    # Reprodutibilidade\n",
        "                verbose=1)          # Mostrar progresso\n",
        "\n",
        "    tsne_results = tsne.fit_transform(features)\n",
        "    # O resultado será (1468, 2)\n",
        "    print(\"Cálculo do t-SNE concluído.\")\n",
        "\n",
        "    # 4. Plotar os resultados\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    for i, class_name in enumerate(classes):\n",
        "        # Encontra os índices (posições) de todas as imagens que pertencem a esta classe\n",
        "        indices = np.where(y_true_labels == i)\n",
        "\n",
        "        # Plota um scatter plot apenas para esses pontos\n",
        "        plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1],\n",
        "                    label=class_name,\n",
        "                    alpha=0.7,\n",
        "                    s=15)\n",
        "\n",
        "    plt.title(f'Visualização t-SNE das Features Extraídas - {MODEL_NAME}')\n",
        "    plt.xlabel('Componente t-SNE 1')\n",
        "    plt.ylabel('Componente t-SNE 2')\n",
        "    plt.legend(markerscale=3) # Legendas com marcadores maiores\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fddk4W8_mMNL",
        "outputId": "b9bcc3f9-2c03-4815-f2c5-00f51027fd9f"
      },
      "outputs": [],
      "source": [
        "# --- 18. Carregar o Modelo Final ---\n",
        "print(\"\\n--- INICIANDO FASE FINAL: AVALIAÇÃO ---\")\n",
        "\n",
        "# Use o nome do arquivo da Fase 2 (fine-tuned)\n",
        "MODEL_PATH = '/content/drive/MyDrive/dataset/diatom_classifier_best_model_finetuned.keras'\n",
        "MODEL_NAME = 'Modelo Tratado 1.0 (99.02%)' # Dê um nome para os gráficos\n",
        "\n",
        "print(f\"Carregando o modelo final de: {MODEL_PATH}\")\n",
        "model = load_model(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e8GyUJXQiqfg",
        "outputId": "666decfc-62e5-49bb-9b64-ef079a23c5d6"
      },
      "outputs": [],
      "source": [
        "# --- 19. Avaliação Final Simples ---\n",
        "print(\"\\nAvaliando o modelo final no dataset de validação...\")\n",
        "# (val_dataset, val_files, BATCH_SIZE vêm das células anteriores)\n",
        "validation_steps = math.ceil(len(val_files) / BATCH_SIZE)\n",
        "results = model.evaluate(val_dataset, steps=validation_steps, verbose=1)\n",
        "\n",
        "print(\"\\nResultados da Avaliação Final:\")\n",
        "print(f\"  Perda (Loss):    {results[0]:.4f}\")\n",
        "print(f\"  Acurácia (Acc): {results[1]*100:.2f}%\")\n",
        "\n",
        "# --- 20. Obter Predições vs. Labels Reais (para as próximas células) ---\n",
        "print(\"\\nGerando predições em todo o dataset de validação...\")\n",
        "\n",
        "# Recria os labels one-hot (caso o notebook tenha sido reiniciado)\n",
        "val_labels_one_hot = tf.keras.utils.to_categorical(val_labels, num_classes=len(CLASSES))\n",
        "# Recria o dataset de validação (sem shuffle)\n",
        "val_dataset_eval = create_dataset(val_files, val_labels_one_hot, is_training=False)\n",
        "\n",
        "# Listas para armazenar os resultados\n",
        "y_pred_proba_list = []\n",
        "y_true_labels_list = []\n",
        "\n",
        "# Itera pelo dataset para pegar predições e labels\n",
        "for images, labels in val_dataset_eval:\n",
        "    y_pred_proba_list.append(model.predict(images, verbose=0))\n",
        "    y_true_labels_list.extend(np.argmax(labels.numpy(), axis=1))\n",
        "\n",
        "# Converte as listas em arrays NumPy\n",
        "y_pred_proba = np.concatenate(y_pred_proba_list, axis=0) # Probabilidades (ex: [0.1, 0.9, ...])\n",
        "y_true_labels = np.array(y_true_labels_list)           # Labels de índice (ex: [1, 1, ...])\n",
        "y_true_one_hot = val_labels_one_hot                      # Labels One-Hot (para ROC)\n",
        "y_pred_labels = np.argmax(y_pred_proba, axis=1)        # Labels de índice (ex: [1, 2, ...])\n",
        "\n",
        "print(\"Predições concluídas e armazenadas em variáveis.\")\n",
        "\n",
        "# --- 21. Relatório de Classificação ---\n",
        "print(\"\\n--- Relatório de Classificação (Precision, Recall, F1-Score) ---\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=CLASSES))\n",
        "\n",
        "# --- 22. Matriz de Confusão ---\n",
        "print(\"\\nGerando Matriz de Confusão...\")\n",
        "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "plt.title(f'Matriz de Confusão - Acurácia: {results[1]*100:.2f}% - {MODEL_NAME}')\n",
        "plt.ylabel('Classe Verdadeira (True Label)')\n",
        "plt.xlabel('Classe Prevista (Predicted Label)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "id": "Ww9HjKuMl_0K",
        "outputId": "3708a78f-7ef0-4c3a-db99-0fa03ac8bc70"
      },
      "outputs": [],
      "source": [
        "# --- Curvas ROC e AUC ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- PASSO 9: Gerando Curvas ROC e AUC (One-vs-Rest) ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Dicionários para armazenar as taxas\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "n_classes = len(CLASSES)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Binariza os labels verdadeiros (necessário para One-vs-Rest)\n",
        "# y_true_one_hot já foi carregado na célula anterior\n",
        "\n",
        "# Calcula a curva ROC e AUC para cada classe\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_one_hot[:, i], y_pred_proba[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], lw=2,\n",
        "             label=f'Classe {CLASSES[i]} (AUC = {roc_auc[i]:.4f})')\n",
        "\n",
        "# Plota a linha de \"chute aleatório\"\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.50)')\n",
        "\n",
        "# Formatação do gráfico\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos (1 - Especificidade)')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos (Recall)')\n",
        "plt.title(f'Curvas ROC (One-vs-Rest) - {MODEL_NAME}')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hw9dT2J6mVL5",
        "outputId": "a72e9ebf-3579-438d-b73a-41f5061f832d"
      },
      "outputs": [],
      "source": [
        "# --- Visualização t-SNE ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- PASSO 10: Gerando Visualização t-SNE ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Criar o \"extrator de features\"\n",
        "# Usamos o nome 'global_average_pooling2d' que demos ao nosso modelo\n",
        "try:\n",
        "    feature_layer = model.get_layer('global_average_pooling2d')\n",
        "except ValueError:\n",
        "    print(\"ERRO: Não foi possível encontrar a camada 'global_average_pooling2d'.\")\n",
        "    print(\"Certifique-se de que você nomeou a camada na Célula 9 e RE-TREINOU o modelo.\")\n",
        "\n",
        "if 'feature_layer' in locals():\n",
        "    # Cria um novo modelo que termina na camada de features\n",
        "    feature_extractor = Model(inputs=model.input, outputs=feature_layer.output)\n",
        "\n",
        "    # 2. Extrair as features (embeddings) de todo o dataset de validação\n",
        "    print(\"Extraindo features (embeddings) do dataset de validação...\")\n",
        "    # val_dataset_eval foi criado na célula de avaliação\n",
        "    features = feature_extractor.predict(val_dataset_eval, verbose=1)\n",
        "    print(f\"Features extraídas. Shape: {features.shape}\")\n",
        "\n",
        "    # 3. Rodar o t-SNE\n",
        "    print(\"Calculando t-SNE (isso pode levar alguns minutos)...\")\n",
        "    tsne = TSNE(n_components=2,       # Reduzir para 2D\n",
        "                perplexity=30.0,    # Padrão\n",
        "                n_iter=1000,\n",
        "                random_state=42,\n",
        "                verbose=1)\n",
        "\n",
        "    tsne_results = tsne.fit_transform(features)\n",
        "    print(\"Cálculo do t-SNE concluído.\")\n",
        "\n",
        "    # 4. Plotar os resultados\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    for i, class_name in enumerate(CLASSES):\n",
        "        # Encontra os índices (posições) de todas as imagens que pertencem a esta classe\n",
        "        # y_true_labels foi calculado na célula de avaliação\n",
        "        indices = np.where(y_true_labels == i)\n",
        "\n",
        "        plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1],\n",
        "                    label=class_name,\n",
        "                    alpha=0.7,\n",
        "                    s=15)\n",
        "\n",
        "    plt.title(f'Visualização t-SNE das Features Extraídas - {MODEL_NAME}')\n",
        "    plt.xlabel('Componente t-SNE 1')\n",
        "    plt.ylabel('Componente t-SNE 2')\n",
        "    plt.legend(markerscale=3)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
